ğŸš€ Crypto Analytics System â€“ Real-Time Binance Tracking & Analysis Application

This project is a desktop application that tracks cryptocurrency data from Binance in real time, analyzes market trends, visualizes token price history, and stores all data in MongoDB Atlas for persistent storage.

The system is built using Tkinter, Matplotlib, Requests, MongoEngine, Pydantic, and Scrapy.
The project can also be compiled into a Windows executable (.exe) for systems without Python installed.

1ï¸âƒ£ Project Purpose

The goal of this project is to:

Track the top N Binance USDT pairs in real time

Provide users with live price, percentage change, and volume data

Generate market analysis reports (top gainers and losers)

Show detailed views and price charts for selected tokens

Persist token snapshots and historical price records in MongoDB

Perform additional data scraping via Scrapy

This application demonstrates the fundamental components of a crypto market analytics tool.

2ï¸âƒ£ Use Cases
ğŸ”¹ 2.1 â€” Real-Time Market Tracking

When the user clicks START STREAM:

The system retrieves live market data from Binance

Displays:

Price

24h percentage change

Session percentage change (since stream start)

24h volume

The table continuously updates during the session.

ğŸ”¹ 2.2 â€” Market Analysis (Gainers / Losers)

When the ANALYZE button is pressed, the system generates a report containing:

Top 5 gainers

Top 5 losers

Total number of analyzed assets

Timestamp of the analysis

This summary is displayed inside the analysis panel.

ğŸ”¹ 2.3 â€” Token Detail & Price Chart

When a token is selected (or double-clicked):

A separate window opens showing:

Token ID, name, price, volume, 24h change, category

A price chart of the last ~30 days

A summary of the last 5 days

This helps visualize short-term market trends.

ğŸ”¹ 2.4 â€” MongoDB Atlas Data Storage

The application saves all fetched data to MongoDB:

TokenDocument â€” current token snapshot

HistoricalDocument â€” historical price records

When the program is restarted, previously saved token data is loaded automatically and displayed in the table.

ğŸ”¹ 2.5 â€” Scrapy-Based Data Collection

The RUN SCRAPY button:

Dynamically generates a Scrapy spider (binance_spider.py)

Executes it using:

python -m scrapy runspider binance_spider.py -o binance_data.json


Saves the output into binance_data.json inside the project directory

This JSON file can be used for offline analysis or dataset preparation.

3ï¸âƒ£ Project Structure

The project uses a flat directory layout as required:

.
â”œâ”€â”€ core.py         # API requests, MongoDB models, data classes, analytics
â”œâ”€â”€ main.py         # Application entry point (launches Tkinter GUI)
â”œâ”€â”€ ui.py           # TK interface, tables, charts, Scrapy integration
â”œâ”€â”€ main.exe        # Compiled Windows executable
â”œâ”€â”€ test.py         # Test file (optional)
â””â”€â”€ README.md       # Documentation

4ï¸âƒ£ How to Run the Application
ğŸ”¹ 4.1 â€” Running from Source (Python)
Requirements:

Python 3.10+

Internet connection

MongoDB Atlas account (or local MongoDB)

Install dependencies:
pip install pymongo mongoengine pydantic requests scrapy dnspython matplotlib


If using a virtual environment:

python -m venv venv
venv\Scripts\activate

Start the application:
python main.py --limit 50

ğŸ”¹ 4.2 â€” Running via Executable (Windows)

Launch the program by double-clicking:

main.exe


Python installation is not required.

To generate your own executable:

pyinstaller --onefile --noconsole main.py


The executable will be located in:

/dist/main.exe

5ï¸âƒ£ Known Issues & Limitations
Issue	Description
Binance API rate limits	Excessive request frequency may cause temporary throttling.
Network / DB failures	Internet or MongoDB outages produce read/write errors.
Scrapy dependency	Scrapy must be installed for JSON export to work.
MongoDB URI inside code	For demonstration only; production requires environment variables.
Missing historical data	When API history is unavailable, a flat synthetic dataset is shown.

Despite these limitations, the system operates reliably for demonstration and coursework purposes.